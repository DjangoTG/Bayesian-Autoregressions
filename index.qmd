---
title: "Bayesian Autoregressions"
authors:
  - name: Tomasz Woźniak
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
  - name: Thomas Kronholm Møller
  - name: Jonas Loopers Davidsen

execute:
  echo: false
  
citation: 
  issued: 2023-05-25
  url: https://donotdespair.github.io/Bayesian-Autoregressions/
  doi: 10.26188/23255657
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for autoregressive models. The range of topics includes the natural conjugate analysis using normal-inverted-gamma 2 prior distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We focus on forecasting and sampling from the predictive density.
>
> **Keywords.** Autoregressions, Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Natural Conjugacy, Shrinkage Prior

# Autoregressions

Autoregressions are a popular class of linear models that are the most useful for time series persistence analysis and forecasting a random variable's unknown future values. The simplicity of their formulation, estimation, and range of applications in which they occur useful decides on their continued employment. 

## The AR($p$) model

The model is set for a univariate time series whose observation at time $t$ is denoted by $y_t$. It includes a $d$-vector $d_t$ of deterministic terms and $p$ lags of the dependent variable on the right-hand side of the model equation. It is complemented by error term $u_t$ that, in this note, is zero-mean normally distributed with variance $\sigma^2$. Then the model equations are:
\begin{align}
y_t &= \alpha_d' d_t + \alpha_1 y_{t-1} + \dots + \alpha_p y_{t-p} + u_t\\
u_t\mid d_t, y_{t-1}, \dots, y_{t-p} &\sim\mathcal{N}\left(0, \sigma^2\right)
\end{align}
where $\alpha_d$ is a $d$-vector of coefficients on deterministic terms, and parameters $\alpha_1,\dots,\alpha_p$ are autoregressive slopes.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. Define a $T$-vector of zeros, $\mathbf{0}_T$, the identity matrix of order $T$, $\mathbf{I}_T$, $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots \\ y_T\end{bmatrix}, \quad
\text{ and }\quad
\mathbf{u} = \begin{bmatrix} u_1\\ \vdots \\ u_T\end{bmatrix},
\end{align}
a $k\times1$ vector $\mathbf{x}_t = \begin{bmatrix}d_t' & y_{t-1}&\dots& y_{t-} \end{bmatrix}'$, where $k=d+p$, and a $T\times k$ matrix collecting the explanatory variables:
\begin{align}
\mathbf{X} = \begin{bmatrix} \mathbf{x}_1'\\ \vdots \\ \mathbf{x}_T'\end{bmatrix}.
\end{align}
Collect the parameters of the conditional mean equation in a $k$-vector:
\begin{align}
\boldsymbol\alpha = \begin{bmatrix} \alpha_d'& \alpha_1 & \dots & \alpha_p\end{bmatrix}'.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{X} \boldsymbol\alpha + \mathbf{u}\\
\mathbf{u}\mid \mathbf{X} &\sim\mathcal{N}_T\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector $\mathbf{y}$. To see this, consider the model equation as a linear transformation of a normal vector $\mathbf{u}$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \mathbf{X}, \boldsymbol\alpha, \sigma^2 &\sim\mathcal{N}_T\left(\mathbf{X} \boldsymbol\alpha, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that is defined as the sampling data density:
\begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X})\equiv p\left(\mathbf{y}\mid \mathbf{X}, \boldsymbol\alpha, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of matrices $\mathbf{y}$ and $\mathbf{X}$, is considered a function of parameters $\boldsymbol\alpha$ and $\sigma^2$ is given by:
\begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)'(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)\right\}.
\end{align}


# Natural-Conjugate Analysis

> **Authors:** Thomas Kronholm Møller & Jonas Loopers Davidsen

## Likelihood as normal-inverted gamma 2

In order to facilitate deriving the posterior distribution, the likelihood function can be rewritten to a $\mathcal{NIG}2$-distribution. For the sake of conciseness the pdf of the $\mathcal{NIG}2$-distribution will not be stated. However, the moments of the $\mathcal{NIG}2(\mu,\Sigma,s,\nu)$-distribution have to satisfy the following conditions:

\begin{gather}
E\left[X\right]=\mu,\;\;\textrm{for}\;\; \nu > 1,\;\;Var\left(X\right)=\frac{s}{\nu-2}\Sigma,\;\;\textrm{for}\;\;\nu>2 \\
E\left[\sigma^2\right]=\frac{s}{\nu-2},\;\;\textrm{for}\;\;\nu>2,\;\;Var(\sigma^2)=\frac{2}{\nu-4}\left[E\left[\sigma^2\right] \right]^2,\;\;\textrm{for}\;\;\nu > 4
\end{gather} For the derivation, note that we only consider the kernel of the likelihood. Further, please note that we are dealing with a univariate autoregression and therefore $N=1$:

```{=tex}
\begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X}) &\propto
\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)'(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)\right\}\\
&\propto \left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}+\mathbf{X}\boldsymbol{\hat{\alpha}} - \mathbf{X}\boldsymbol\alpha)'(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}+\mathbf{X}\boldsymbol{\hat{\alpha}} - \mathbf{X}\boldsymbol\alpha)\right\}\\
&\propto \left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\left[(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}})'\mathbf{X}'\mathbf{X}(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}})+(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}})'(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}})\right]\right\}\\
&\propto \left(\sigma^2\right)^{-\frac{T-3+1+2}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\left(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}}\right)'\mathbf{X}'\mathbf{X}\left(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}}\right) \right\}+\left\{-\frac{1}{2}\frac{1}{\sigma^2}\left(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}\right)'\left(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}\right) \right\}
\end{align}
```
It is now quite straight forward to identify the $\mathcal{NIG}2$-kernel. By remembering that the $\mathcal{NIG}2$-distribution is characterized by its four moments, we get the following outcome:

```{=tex}
\begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X}) = \mathcal{NIG}2\left(\mu=\boldsymbol{\hat{\alpha}},\Sigma=\left(\mathbf{X}'\mathbf{X}\right)^{-1},s=\left(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}\right)'\left(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}\right),\nu=T-3\right)
\end{align}
```

## Normal-inverted gamma 2 prior

The prior distribution of the natural conjugate is determined by the form of the distribution of the parameters implied by the likelihood function discussed above. The priors for the Normal-inverse gamma 2 distribution can thus be written as:

```{=tex}
\begin{align}
p(\boldsymbol\alpha,\sigma^2) &= p(\boldsymbol\alpha,\sigma^2)P(\sigma^2)\\
      p(\boldsymbol\alpha,\sigma^2) &= \mathcal{N}(\underline{\boldsymbol\alpha},\sigma^2\underline{\sigma}^2_{\boldsymbol\alpha})\\
      p(\sigma^2) & = \mathcal{IG}2(\underline{s},\underline{\nu})
\end{align}
```
Using the distributions above, we can write the kernel of the $\mathcal{NIG}2$ as:

```{=tex}
\begin{align}
\mathcal{NIG}2_{N=1}(\underline{\boldsymbol\alpha},\underline{\sigma}^2_{\boldsymbol\alpha}, \underline{s}, \underline{\nu}) \propto (\sigma^2)^{\frac{-\underline{\nu}+3}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\frac{1}{\underline{\sigma}^2}_{\boldsymbol\alpha}(\boldsymbol\alpha-\underline{\boldsymbol\alpha})'(\boldsymbol\alpha-\underline{\boldsymbol\alpha})\right\}\exp\left\{-\frac{1}{2}\frac{\underline{s}}{\sigma^2}\right\}
\end{align}
```
## Normal-inverted gamma 2 posterior

The product of the prior distribution and the likelihood function as introduced above gives the posterior distribution given by:

```{=tex}
\begin{align}
p(\boldsymbol\alpha,\sigma^2|\mathbf{y},\mathbf{X}) \propto L(\mathbf{y}|\mathbf{X}, \boldsymbol\alpha, \sigma^2)p(\boldsymbol\alpha,\sigma^2) =  L(\mathbf{y}|\mathbf{X}, \boldsymbol\alpha, \sigma^2)p( \boldsymbol\alpha| \sigma^2)p(\sigma^2)
\end{align}
```
Inserting the respective distributions gives us the kernel which allows us to derive the posterior distributions:

```{=tex}
\begin{align}
p(\boldsymbol\alpha,\sigma^2|\mathbf{y},\mathbf{X}) & \propto \left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\left[(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}})'\mathbf{X}'\mathbf{X}(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}})+(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}})'(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}})\right]\right\} \\ & \times (\sigma^2)^{-\frac{\underline{\nu}+3}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\frac{1}{\underline{\sigma}^2_{\boldsymbol\alpha}}(\boldsymbol\alpha-\underline{\boldsymbol\alpha})'(\boldsymbol\alpha-\underline{\boldsymbol\alpha})\right\}\exp\left\{-\frac{1}{2}\frac{\underline{s}}{\sigma^2}\right\}
\end{align}
```
Now, in order to find the posterior distribution, we start by collecting the terms of $\sigma^2$:

```{=tex}
\begin{align}
p(\boldsymbol\alpha,\sigma^2|\mathbf{y},\mathbf{X}) & \propto (\sigma^2)^{-\frac{T+\underline{\nu}+3}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\left[\frac{1}{\underline{\sigma}^2}_{\boldsymbol\alpha} (\boldsymbol\alpha-\underline{\boldsymbol\alpha})'(\boldsymbol\alpha-\underline{\boldsymbol\alpha}) + (\boldsymbol\alpha-\boldsymbol{\hat{\alpha}})'\mathbf{X}'\mathbf{X}(\boldsymbol\alpha-\boldsymbol{\hat{\alpha}})+(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}})'(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\alpha}}) + \underline{s}  \right] \right\}
\end{align}
```
Then, in order to "complete" the squares, we expand the expression found in the square brackets by multiplying all the elements and collect the ones containing $\boldsymbol\alpha$ and $\boldsymbol\alpha^2$:

```{=tex}
\begin{align}
\boldsymbol\alpha^2(\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\mathbf{X}'\mathbf{X})-\boldsymbol\alpha  2 (\underline{\boldsymbol\alpha}\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\boldsymbol{\hat{\alpha}}\mathbf{X}'\mathbf{X})+\underline{\boldsymbol\alpha}^2\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{s}+\mathbf{y}'\mathbf{y}
\end{align}
```
We can now by defining $\overline{\sigma}^{-2}_{\boldsymbol\alpha}=(\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\mathbf{X}'\mathbf{X})$ rewrite the expression, where the second term has been multiplied and divided by $\overline{\sigma}^{2}_{\boldsymbol\alpha}$:

```{=tex}
\begin{align}
\boldsymbol\alpha^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}-\boldsymbol\alpha 2 (\underline{\boldsymbol\alpha} \underline{\sigma}^{-2}_{\boldsymbol\alpha}+\boldsymbol{\hat{\alpha}}\mathbf{X}'\mathbf{X})\overline{\sigma}^{2}_{\boldsymbol\alpha}\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{\boldsymbol\alpha}^2\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{s}+\mathbf{y}'\mathbf{y}
\end{align}
```
By further defining $\overline{\boldsymbol\alpha}=(\underline{\boldsymbol\alpha} \underline{\sigma}^{-2}_{\boldsymbol\alpha}+\boldsymbol{\hat{\alpha}}\mathbf{X}'\mathbf{X})\overline{\sigma}^{2}_{\boldsymbol\alpha}$ the function can be rewritten as:

```{=tex}
\begin{align}
\boldsymbol\alpha^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}-\boldsymbol\alpha 2\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}-\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{\boldsymbol\alpha}^2\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{s}+\mathbf{y}'\mathbf{y}
\end{align}
```
where we have added and subtracted $\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}$.

One can now recognize that the first three terms, thus $\boldsymbol\alpha^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}-\boldsymbol\alpha 2\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}$, can be expressed as $\overline{\sigma}^{-2}_{\boldsymbol\alpha}\left(\boldsymbol\alpha-\overline{\boldsymbol\alpha}\right)'\left(\boldsymbol\alpha-\overline{\boldsymbol\alpha}\right)$. Having derived this, we can now state the final expression for the term in the square brackets as the following:

```{=tex}
\begin{align}
\overline{\sigma}^{-2}_{\boldsymbol\alpha}\left(\boldsymbol\alpha-\overline{\boldsymbol\alpha}\right)'\left(\boldsymbol\alpha-\overline{\boldsymbol\alpha}\right)-\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{\boldsymbol\alpha}^2\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{s}+\mathbf{y}'\mathbf{y}
\end{align}
```
By plugging this expression back into the initial joint posterior distribution we compute the following:
```{=tex}
\begin{align}
p(\boldsymbol\alpha,\sigma^2|\mathbf{y},\mathbf{X}) &\propto (\sigma^2)^{-\frac{T+\underline{\nu}+3}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\frac{1}{\underline{\sigma}^2}_{\boldsymbol\alpha} (\boldsymbol\alpha-\underline{\boldsymbol\alpha})'(\boldsymbol\alpha-\underline{\boldsymbol\alpha})\right\} \times \exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\left(-\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{\boldsymbol\alpha}^2\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{s}+\mathbf{y}'\mathbf{y}\right)\right\}
\\&= (\sigma^2)^{\frac{\overline{\nu}+3}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}\frac{1}{\underline{\sigma}^2}_{\boldsymbol\alpha} (\boldsymbol\alpha-\underline{\boldsymbol\alpha})'(\boldsymbol\alpha-\underline{\boldsymbol\alpha})\right\} \times \exp\left\{-\frac{1}{2}\frac{\overline{s}}{\sigma^2}\right\}
\end{align}
```
This now fully defines the joint posterior distribution as is it is in a normal inverse gamma 2 form with its corresponding four moments:

```{=tex}
\begin{align}
p(\boldsymbol\alpha,\sigma^2|\mathbf{y},\mathbf{X}) &= \mathcal{NIG}2_{(N=1)}\left(\overline{\boldsymbol\alpha},\overline{\sigma}^2_{\boldsymbol\alpha},\overline{s},\overline{\nu}\right)\\
\overline{\boldsymbol\alpha} &= (\underline{\boldsymbol\alpha} \underline{\sigma}^{-2}_{\boldsymbol\alpha}+\boldsymbol{\hat{\alpha}}\mathbf{X}'\mathbf{X})\overline{\sigma}^{2}_{\boldsymbol\alpha}\\
\overline{\sigma}^2_{\boldsymbol\alpha} &=\left(\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\mathbf{X}'\mathbf{X}\right)^{-1} \\
\overline{s} &= -\overline{\boldsymbol\alpha}^2\overline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{\boldsymbol\alpha}^2\underline{\sigma}^{-2}_{\boldsymbol\alpha}+\underline{s}+\mathbf{y}'\mathbf{y} \\
\overline{\nu} &= \underline{\nu}+T 
\end{align}
```


## Sampling draws from the posterior

We start by generating a random walk, which can be used to validate that our estimation is indeed correct.

```{r}
#| echo: true
#| message: false
#| warning: false
T       = 500
N       = 1
y       = apply(matrix(rnorm(T * N), ncol = N), 2, cumsum)
y       = as.matrix(y)
p       = 1

T     = nrow(y)
Y     = as.matrix(y[(p+1):T,])
X     = cbind(rep(1,T - p),y[1:(T - p),])

```

Now, defining our priors for $\underline{\boldsymbol\alpha}$, $\underline{\sigma}^2_{\boldsymbol\alpha}$, $\underline{s}$ and $\underline{\nu}$:

```{r}
#| echo: true
#| message: false
#| warning: false

priors = list(
  alpha   = as.matrix(c(0,0)),
  Sigma   = diag(2),
  S       = 1,
  nu      = 3
)
```

and computing the function for the posterior parameters:

```{r}
#| echo: true
#| message: false
#| warning: false
fin.posterior = function(y,priors){
  Sigma.inv = t(X) %*% X + priors$Sigma
  Sigma     = solve(Sigma.inv)
  alpha     = Sigma %*% (t(X) %*% Y + solve(priors$Sigma) %*% priors$alpha)
  S         = as.numeric(t(Y) %*% Y + priors$S + t(priors$alpha) %*% solve(priors$Sigma) %*% priors$alpha 
                     - t(alpha) %*% Sigma.inv %*% alpha)
  nu        = T - 1 + priors$nu
  
  return   (list(
    Sigma  = Sigma,
    alpha  = alpha,
    S      = S,
    nu     = nu
  ))
}

post = fin.posterior(y=y,priors=priors)
```

We are then able to do the estimation of our parameters using the Gibbs sampler provided below.

```{r}
#| echo: true
#| message: false
#| warning: false
fin.posterior.draws = function(S,posterior){
  Sigma2.posterior  = as.matrix(posterior$S/rchisq(S,posterior$nu))
  alpha.posterior       = simplify2array(
    lapply(1:S,function(i){
      mvtnorm::rmvnorm(1,mean=posterior$alpha,sigma=Sigma2.posterior[i,]*posterior$Sigma)
    })
  )
  output = cbind(t(alpha.posterior[1,,]),Sigma2.posterior)
  return(output)
}

draws = fin.posterior.draws(S=1000,posterior=post)
```




# Hierarchical Prior Analysis

## Estimating autoregressive prior shrinkage

### Inverted gamma 2 scale mixture of normal

### Gamma scale mixture of normal

## Estimating error term variance prior scale

## Dummy observation prior




# Model Extensions

## Student-$t$ error term

## Estimating autoregressions after 2020

## Stochastic volatility heteroskedasticity



# Forecasting

## Conditional predictive density

## Algorithm to sample from the predictive density

## Sampler implementation in R


# References {.unnumbered}